\documentclass[a4paper,10pt]{article}

\usepackage[dvips]{graphicx}
\usepackage[a4paper,left=3cm,right=3cm,top=4cm,bottom=4cm]{geometry}
\usepackage{amssymb,amsmath,amsthm,stmaryrd}
\usepackage{indentfirst}
\usepackage{latexsym}
\usepackage[dvipsnames]{xcolor}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{eurosym}

\usetikzlibrary{arrows,shapes,positioning}

\newcommand{\vname}[1]{\textcolor{OliveGreen}{\texttt{#1}}}
\newcommand{\typename}[1]{\textcolor{Blue}{\texttt{#1}}}

\begin{document}
\large
\begin{center}
\textsc{Lekta documents} \\

\LARGE
\vspace{24pt}
\textbf{Lekta Tutorial and Documentation}
\vspace{24pt}
\end{center}

\normalsize
\begin{flushleft}
\emph{Document Type:} Tutorial \\
\emph{Date:} 2016/09/26 \\
\emph{Version:} 0.1
\end{flushleft}

\vspace{24pt}
\section{Possible index of Lekta tutorial}

\begin{enumerate}
	\item Introduction
	\begin{enumerate}
		\item What is Lekta?
		\item Features of Lekta Programming Language
	\end{enumerate}
	\item Your first Lekta project
		\begin{enumerate}
			\item Setting up a lekta project
			\item Folder and file structure
		\end{enumerate}
	\item Automatic Speech Recognition
	\item Natural Language Understanding
	\begin{enumerate}
		\item Lexicons
		\item Grammars
	\end{enumerate}
	\item Dialogue Manager
	\begin{enumerate}
		\item Colligo
		\item Senso
		\item MindBoard
		\item Respondo
		\item Locutio
	\end{enumerate}
	\item Natural Language Generation
	\begin{enumerate}
		\item Templates
	\end{enumerate}
	\item Text to Speech Synthesis
	\item Programmer Reference
	\begin{enumerate}
		\item Metatypes
		\item Declaring new types in Lekta
		\item Basic programming structures
		\item Lekta own commands 
		\item Built-in functions
	\end{enumerate}
\end{enumerate}

\section{Introduction}
Language: Set of conventional spoken or written symbols used for commucation between entities.
So we can see language as the linking between meaning (semantic aspect) and expression (syntantic aspect)

We have two types of languages:

* Natural languages: used for the communication between human beings (indeed the most natural form of communication of humans).

* Formal languages: used by computers and in mathematical areas.

So we have some distintions between these two types of languages:

* Computers don't understand natural language. (Normal) humans don't understand computer languages.

* Formal languages mustn't have ambiguities but natural languages do have (some examples in a minute).

Natural Language Processing (NLP) and, by the way, what we call Language Technologies want to create some software that have some knowledge about natural languages.

NLP started with the techniques used to study formal languages and adopted (also adapted) its techniques to be able to deal with natural language.

Applications of LT's:

* Machine Translation.

* Q\&A = Question Answering (Siri, Cortana).

* Automatic text classification (topic extraction).

* Automatic text summarization.

* Social analytics of comments in Facebook and Twitter. This topic is in fashion due to their succesful predictions in some events like Eurovision song contest and 
opinion trending media in US president election of Donald Trump.

* Dialogue Systems (Spoken and Written). Our main field of action. Also known as conversational interfaces or chatbots.

Applications of conversational interfaces:

* Information providers. Account balance, search of available flights, ...

* Transactional providers. Banking transactions, flight, hotel or medical appointments booking.

* Educational. Tutoring in the learning of some subjects (or indeed language learning like ELEna application).



The most thrilling challenge in the design of conversational interfaces are related with the highly ambigous nature of natural language. These ambiguities are present in almost every stage of natural language processing.

Examples:

2 syntantic forms have 1 semantic form

Peter come yesterday.

Yesterday Peter come.

1 syntantic form has 2 semantic forms

Peter said John came yesterday.

Was it yesterday when Peter said that ...?

Was it yesterday that John came ...?

Humans can deal with this kind of ambiguities applying psicolinguistic preferences and, sometimes, logic and reasoning. For example, the next sentence is as same as ambigous than the previous one from the point of view of a computer, but everybody knows, without effort, what occurred yesterday.

Peter said John will come yesterday

We will be centered in conversational interfaces. The most used and important diagram of the architecture is this:

asr - nlu - dm - nlg - tts - database - backoffice - web services

\section{Natural Language Understanding (NLU)}

The goal of SLU stage is to transform an input string, possibly an user proference, in an abstract representation of the meaning of that proference. 

This kind of representation must be computationally affordable by the computer in order to execute some kind of reasoning.

For example:

John came yesterday 

SUBJECT: John

ACTION: come

TENSE: past

OFFSETDATE: -1 day

John will talk in two days

SUBJECT: John

ACTION: talk

TENSE: future

OFFSETDATE: +2 days

The NLU stage can be divided into some smaller parts:

* Tokenization.

* Speller checker.

* POS Tagging.

* Parsing.

* Unifier.

Tokenizer

Convert a sequence of characters into a sequence of tokens (this is aka lexical analysis). In this part we must take into account:

* Separators ( -\_)

* Punctuation marks (.,;:)

* Special symbols (\$\euro \%?!)

* Numbers (1234)

* Alphanumeric codes

Example:

\texttt{  tk1   tk2   tk3  tk4  tk5   tk6}
  
\texttt{$\cdot$ The $\cdot$ dog $\cdot$ is $\cdot$ in $\cdot$ the $\cdot$ park $\cdot$}

\texttt{0     1     2    3    4     5      6}

Speller Checker






\end{document}
